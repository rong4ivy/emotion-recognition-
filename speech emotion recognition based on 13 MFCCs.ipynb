{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aH3XYIChaQZR",
    "outputId": "e1bd5346-5517-4b36-c59b-3e6752faa590"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "train_file = 'train.json'\n",
    "with open(train_file, 'r') as f:\n",
    "        df=pd.read_json(f)\n",
    "        \n",
    "df=df.T\n",
    "features=df[\"features\"].values\n",
    "valence = df[\"valence\"].values\n",
    "activation = df[\"activation\"].values\n",
    "\n",
    "all_lengths = [len(seq) for seq in features]\n",
    "max_length = max(all_lengths)\n",
    "x1 = [x for x in all_lengths if x > 300 and x <= 500]\n",
    "x2 = [x for x in all_lengths if x > 500]\n",
    "x3 = [x for x in all_lengths if x > 00]\n",
    "print(len(x1), len(x2),len(x3))\n",
    "\n",
    "average_length = np.mean(all_lengths)\n",
    "print(average_length,max_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Y_Lwatltbi7f"
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def load_data(train_file):\n",
    "    with open(train_file, 'r') as f:\n",
    "         df=pd.read_json(f)\n",
    "    df=df.T\n",
    "    valence = df[\"valence\"].values\n",
    "    activation = df[\"activation\"].values\n",
    "    features = df[\"features\"].values\n",
    "    lengths = [len(seq) for seq in features]\n",
    "    desired_seq_length = 500\n",
    "    padded_x = np.zeros((len(features), desired_seq_length, len(features[0][0])))\n",
    "    # loop through each sequence in x\n",
    "    for i, seq in enumerate(features):\n",
    "    # check if sequence length is greater than desired length\n",
    "         if len(seq) > desired_seq_length:\n",
    "            padded_x[i] = seq[:desired_seq_length]\n",
    "         else:\n",
    "            padded_x[i, :len(seq), :] = seq\n",
    "    # sequence length is already equal to desired length\n",
    "\n",
    "    train_data_X = torch.from_numpy(padded_x).float()\n",
    "\n",
    "    # Define the label dictionary\n",
    "    label_dict = {(1, 1): 'joy', (1, 0): 'pleasure', (0, 0): 'sadness', (0, 1): 'anger'}\n",
    "    # Define a function to map valence and activation to label\n",
    "    def map_label(row):\n",
    "        return label_dict[(row['valence'], row['activation'])]\n",
    "    # Add a new column with the label for each row\n",
    "    df['label'] = df.apply(map_label, axis=1)\n",
    "    # Drop the valence and activation columns\n",
    "    df.drop(['valence', 'activation'], axis=1, inplace=True)\n",
    "    # create a dictionary to map the emotion labels to numbers\n",
    "    emotion_dict = {'joy': 0, 'pleasure': 1, 'sadness': 2, 'anger': 3}\n",
    "    # replace the emotion labels with numbers in the dataframe\n",
    "    df['label'] = df['label'].replace(emotion_dict)\n",
    "\n",
    "    labels = df[\"label\"].values\n",
    "    train_data_y = torch.from_numpy(labels)\n",
    "\n",
    "    return train_data_X,  train_data_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dev.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/rongwang/Desktop/ ESR_project/ii.ipynb 单元格 3\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rongwang/Desktop/%20ESR_project/ii.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rongwang/Desktop/%20ESR_project/ii.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dev_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdev.json\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/rongwang/Desktop/%20ESR_project/ii.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(dev_file, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rongwang/Desktop/%20ESR_project/ii.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m          df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mread_json(f)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/rongwang/Desktop/%20ESR_project/ii.ipynb#W2sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m df\u001b[39m=\u001b[39mdf\u001b[39m.\u001b[39mT\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dev.json'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "dev_file = 'dev.json'\n",
    "with open(dev_file, 'r') as f:\n",
    "         df=pd.read_json(f)\n",
    "df=df.T\n",
    "features = df[\"features\"].values\n",
    "lengths = [len(seq) for seq in features]\n",
    "desired_seq_length = 500\n",
    "padded_x = np.zeros((len(features), desired_seq_length, len(features[0][0])))\n",
    "# loop through each sequence in x\n",
    "for i, seq in enumerate(features):\n",
    "    # check if sequence length is greater than desired length\n",
    "      if len(seq) > desired_seq_length:\n",
    "          padded_x[i] = seq[:desired_seq_length]\n",
    "      else:\n",
    "          padded_x[i, :len(seq), :] = seq\n",
    "    # sequence length is already equal to desired length\n",
    "\n",
    "dev_data_X = torch.from_numpy(padded_x).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CGq5nKPvi-Jv"
   },
   "outputs": [],
   "source": [
    "# relatively complicated version with three convs and two lstms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyCnnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "         # Defining a 2D convolution layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=3, stride=2))\n",
    "            # Defining another 2D convolution layer\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 32, kernel_size=3),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=3, stride=2))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "        self.lstm1 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=64, hidden_size=64, batch_first=True)\n",
    "        self.fc1 = nn.Linear(60*64, 32)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(32, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)  # print input shape\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        #print(\"end of CNN\", x.shape)  # print after avgpool3 layer\n",
    "        x=x.squeeze(-1)\n",
    "        x=x.transpose(1,2)\n",
    "        x, _ = self.lstm1(x)\n",
    "        #print(x.shape) \n",
    "        x,_ = self.lstm2(x) \n",
    "        #print(\"after lstm\", x.shape) # print after LSTM layer\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        #print(x.shape) # print after LSTM layer\n",
    "        x = self.fc1(x)\n",
    "        #print(x.shape)  # print after classifier\n",
    "        x = self.activation(x)\n",
    "        x = self.fc2(x)\n",
    "        #print(x.shape)  # print after activation\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MyCnnModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QT6yNODf5hJ6",
    "outputId": "f84674c8-859f-4571-f0d2-5bf84ad45c59"
   },
   "outputs": [],
   "source": [
    "train_file = 'train.json'\n",
    "\n",
    "train_data_X,  train_data_y = load_data(train_file)\n",
    "\n",
    "\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "train_data = Data.TensorDataset(train_data_X, train_data_y)   # merge the features x data and label y into one dataset\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = Data.DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "# print(len(train_loader)) = 244\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyCnnModel()\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KckTKgJZdN_y"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "import time\n",
    "from torch.optim import Adam\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CyclicLR\n",
    "\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, train_loader, train_rate, criterion, optimizer, num_epochs):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    best_acc = 0.0\n",
    "  \n",
    "    batch_num =len(train_loader)\n",
    "    train_batch_num = round(batch_num* train_rate)\n",
    "\n",
    "    train_loss_all = []\n",
    "    train_acc_all = []\n",
    "    val_loss_all = []\n",
    "    val_acc_all = []\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "         print(\"Epoch {}/{}\".format(epoch, num_epochs))\n",
    "         print(\"-\"*10)\n",
    "        \n",
    "       # there are two stages for each epoch\n",
    "         train_loss = 0.0\n",
    "         train_corrects = 0\n",
    "         train_num =0\n",
    "         val_loss = 0.0\n",
    "         val_corrects = 0.0\n",
    "         val_num = 0\n",
    "       \n",
    "         for step, (b_x, b_y) in enumerate(train_loader):\n",
    "              b_x = b_x.to(device)\n",
    "              b_y = b_y.to(device)\n",
    "\n",
    "              if  step < train_batch_num:\n",
    "                  \n",
    "                model.train()\n",
    "\n",
    "                output= model(b_x)\n",
    "                pre_lab = torch.argmax(output, dim=1)\n",
    "                loss = criterion(output, b_y)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item() * b_x.size(0)\n",
    "                train_corrects += torch.sum(pre_lab == b_y.data)\n",
    "                train_num += b_x.size(0)\n",
    "                \n",
    "              else:\n",
    "                \n",
    "                model.eval()\n",
    "                output= model(b_x)\n",
    "                pre_lab = torch.argmax(output, dim=1)\n",
    "                loss = criterion(output, b_y)\n",
    "                val_loss += loss.item() * b_x.size(0)\n",
    "                \n",
    "                val_corrects += torch.sum(pre_lab == b_y.data)\n",
    "                val_num += b_x.size(0)\n",
    "                \n",
    "          \n",
    "         train_loss_all.append(train_loss/train_num)\n",
    "         train_acc_all.append(train_corrects.double().item()/train_num)\n",
    "         val_loss_all.append(val_loss/val_num)\n",
    "         val_acc_all.append(val_corrects.double().item()/val_num)\n",
    "\n",
    "         print('{} Train Loss: {:.4f}, Train Acc: {:.4f}'.format(epoch, train_loss_all[-1], train_acc_all[-1]))\n",
    "         print('{} Val Loss: {:.4f}, Val Acc: {:.4f}'.format(epoch, val_loss_all[-1], val_acc_all[-1]))\n",
    "\n",
    "        \n",
    "        # store the best model with paremeters         # Keep track of the best model weights\n",
    "         if val_acc_all[-1]>best_acc:\n",
    "                       best_acc = val_acc_all[-1]\n",
    "                       best_model_wts = copy.deepcopy(model.state_dict())\n",
    "          \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, train_loss_all, train_acc_all, val_loss_all, val_acc_all\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h1tsePVa0dyI",
    "outputId": "ea3c9676-a1b6-425d-bddb-50b5de0cb94c",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/200\n",
      "----------\n",
      "0 Train Loss: 1.1690, Train Acc: 0.4443\n",
      "0 Val Loss: 1.1131, Val Acc: 0.4658\n",
      "Epoch 1/200\n",
      "----------\n",
      "1 Train Loss: 1.1322, Train Acc: 0.4795\n",
      "1 Val Loss: 1.4050, Val Acc: 0.4737\n",
      "Epoch 2/200\n",
      "----------\n",
      "2 Train Loss: 1.1123, Train Acc: 0.4906\n",
      "2 Val Loss: 1.3612, Val Acc: 0.3355\n",
      "Epoch 3/200\n",
      "----------\n",
      "3 Train Loss: 1.0970, Train Acc: 0.4997\n",
      "3 Val Loss: 1.3004, Val Acc: 0.3711\n",
      "Epoch 4/200\n",
      "----------\n",
      "4 Train Loss: 1.0869, Train Acc: 0.5074\n",
      "4 Val Loss: 1.2048, Val Acc: 0.4961\n",
      "Epoch 5/200\n",
      "----------\n",
      "5 Train Loss: 1.0640, Train Acc: 0.5351\n",
      "5 Val Loss: 1.1849, Val Acc: 0.4434\n",
      "Epoch 6/200\n",
      "----------\n",
      "6 Train Loss: 1.0474, Train Acc: 0.5341\n",
      "6 Val Loss: 1.1741, Val Acc: 0.4750\n",
      "Epoch 7/200\n",
      "----------\n",
      "7 Train Loss: 1.0364, Train Acc: 0.5455\n",
      "7 Val Loss: 1.0776, Val Acc: 0.5158\n",
      "Epoch 8/200\n",
      "----------\n",
      "8 Train Loss: 1.0353, Train Acc: 0.5420\n",
      "8 Val Loss: 1.2012, Val Acc: 0.5013\n",
      "Epoch 9/200\n",
      "----------\n",
      "9 Train Loss: 1.0227, Train Acc: 0.5518\n",
      "9 Val Loss: 1.0636, Val Acc: 0.5224\n",
      "Epoch 10/200\n",
      "----------\n",
      "10 Train Loss: 1.0074, Train Acc: 0.5563\n",
      "10 Val Loss: 0.9837, Val Acc: 0.5684\n",
      "Epoch 11/200\n",
      "----------\n",
      "11 Train Loss: 0.9936, Train Acc: 0.5656\n",
      "11 Val Loss: 1.3554, Val Acc: 0.5013\n",
      "Epoch 12/200\n",
      "----------\n",
      "12 Train Loss: 0.9700, Train Acc: 0.5827\n",
      "12 Val Loss: 0.9343, Val Acc: 0.5882\n",
      "Epoch 13/200\n",
      "----------\n",
      "13 Train Loss: 0.9494, Train Acc: 0.5879\n",
      "13 Val Loss: 1.4134, Val Acc: 0.4658\n",
      "Epoch 14/200\n",
      "----------\n",
      "14 Train Loss: 0.9084, Train Acc: 0.6156\n",
      "14 Val Loss: 1.6529, Val Acc: 0.3803\n",
      "Epoch 15/200\n",
      "----------\n",
      "15 Train Loss: 0.8723, Train Acc: 0.6284\n",
      "15 Val Loss: 0.8761, Val Acc: 0.6171\n",
      "Epoch 16/200\n",
      "----------\n",
      "16 Train Loss: 0.8262, Train Acc: 0.6456\n",
      "16 Val Loss: 0.8260, Val Acc: 0.6303\n",
      "Epoch 17/200\n",
      "----------\n",
      "17 Train Loss: 0.7643, Train Acc: 0.6839\n",
      "17 Val Loss: 0.9119, Val Acc: 0.5934\n",
      "Epoch 18/200\n",
      "----------\n",
      "18 Train Loss: 0.7338, Train Acc: 0.6911\n",
      "18 Val Loss: 0.7492, Val Acc: 0.6803\n",
      "Epoch 19/200\n",
      "----------\n",
      "19 Train Loss: 0.6724, Train Acc: 0.7236\n",
      "19 Val Loss: 0.6840, Val Acc: 0.7184\n",
      "Epoch 20/200\n",
      "----------\n",
      "20 Train Loss: 0.6410, Train Acc: 0.7385\n",
      "20 Val Loss: 0.7570, Val Acc: 0.6632\n",
      "Epoch 21/200\n",
      "----------\n",
      "21 Train Loss: 0.5726, Train Acc: 0.7689\n",
      "21 Val Loss: 0.8086, Val Acc: 0.6789\n",
      "Epoch 22/200\n",
      "----------\n",
      "22 Train Loss: 0.5313, Train Acc: 0.7868\n",
      "22 Val Loss: 0.6116, Val Acc: 0.7513\n",
      "Epoch 23/200\n",
      "----------\n",
      "23 Train Loss: 0.4828, Train Acc: 0.8104\n",
      "23 Val Loss: 0.7034, Val Acc: 0.6987\n",
      "Epoch 24/200\n",
      "----------\n",
      "24 Train Loss: 0.4480, Train Acc: 0.8257\n",
      "24 Val Loss: 0.8418, Val Acc: 0.6882\n",
      "Epoch 25/200\n",
      "----------\n",
      "25 Train Loss: 0.4114, Train Acc: 0.8428\n",
      "25 Val Loss: 0.6941, Val Acc: 0.7211\n",
      "Epoch 26/200\n",
      "----------\n",
      "26 Train Loss: 0.3651, Train Acc: 0.8605\n",
      "26 Val Loss: 0.4013, Val Acc: 0.8487\n",
      "Epoch 27/200\n",
      "----------\n",
      "27 Train Loss: 0.3306, Train Acc: 0.8793\n",
      "27 Val Loss: 0.4889, Val Acc: 0.8066\n",
      "Epoch 28/200\n",
      "----------\n",
      "28 Train Loss: 0.2908, Train Acc: 0.8946\n",
      "28 Val Loss: 0.3088, Val Acc: 0.8974\n",
      "Epoch 29/200\n",
      "----------\n",
      "29 Train Loss: 0.2634, Train Acc: 0.9072\n",
      "29 Val Loss: 0.3579, Val Acc: 0.8671\n",
      "Epoch 30/200\n",
      "----------\n",
      "30 Train Loss: 0.2425, Train Acc: 0.9112\n",
      "30 Val Loss: 0.4278, Val Acc: 0.8329\n",
      "Epoch 31/200\n",
      "----------\n",
      "31 Train Loss: 0.2363, Train Acc: 0.9142\n",
      "31 Val Loss: 0.5750, Val Acc: 0.7855\n",
      "Epoch 32/200\n",
      "----------\n",
      "32 Train Loss: 0.1845, Train Acc: 0.9357\n",
      "32 Val Loss: 0.2440, Val Acc: 0.9118\n",
      "Epoch 33/200\n",
      "----------\n",
      "33 Train Loss: 0.1754, Train Acc: 0.9416\n",
      "33 Val Loss: 0.5567, Val Acc: 0.8013\n",
      "Epoch 34/200\n",
      "----------\n",
      "34 Train Loss: 0.1718, Train Acc: 0.9405\n",
      "34 Val Loss: 1.2348, Val Acc: 0.6842\n",
      "Epoch 35/200\n",
      "----------\n",
      "35 Train Loss: 0.1608, Train Acc: 0.9453\n",
      "35 Val Loss: 0.1971, Val Acc: 0.9316\n",
      "Epoch 36/200\n",
      "----------\n",
      "36 Train Loss: 0.1345, Train Acc: 0.9538\n",
      "36 Val Loss: 0.9110, Val Acc: 0.7289\n",
      "Epoch 37/200\n",
      "----------\n",
      "37 Train Loss: 0.1209, Train Acc: 0.9631\n",
      "37 Val Loss: 0.3068, Val Acc: 0.8882\n",
      "Epoch 38/200\n",
      "----------\n",
      "38 Train Loss: 0.1289, Train Acc: 0.9565\n",
      "38 Val Loss: 0.3917, Val Acc: 0.8908\n",
      "Epoch 39/200\n",
      "----------\n",
      "39 Train Loss: 0.1029, Train Acc: 0.9689\n",
      "39 Val Loss: 0.1809, Val Acc: 0.9303\n",
      "Epoch 40/200\n",
      "----------\n",
      "40 Train Loss: 0.1050, Train Acc: 0.9663\n",
      "40 Val Loss: 0.1113, Val Acc: 0.9566\n",
      "Epoch 41/200\n",
      "----------\n",
      "41 Train Loss: 0.1229, Train Acc: 0.9587\n",
      "41 Val Loss: 0.3109, Val Acc: 0.8882\n",
      "Epoch 42/200\n",
      "----------\n",
      "42 Train Loss: 0.1018, Train Acc: 0.9676\n",
      "42 Val Loss: 0.0756, Val Acc: 0.9789\n",
      "Epoch 43/200\n",
      "----------\n",
      "43 Train Loss: 0.0676, Train Acc: 0.9818\n",
      "43 Val Loss: 0.1236, Val Acc: 0.9513\n",
      "Epoch 44/200\n",
      "----------\n",
      "44 Train Loss: 0.0727, Train Acc: 0.9781\n",
      "44 Val Loss: 0.1889, Val Acc: 0.9289\n",
      "Epoch 45/200\n",
      "----------\n",
      "45 Train Loss: 0.0646, Train Acc: 0.9828\n",
      "45 Val Loss: 0.0694, Val Acc: 0.9816\n",
      "Epoch 46/200\n",
      "----------\n",
      "46 Train Loss: 0.0847, Train Acc: 0.9764\n",
      "46 Val Loss: 0.3846, Val Acc: 0.8553\n",
      "Epoch 47/200\n",
      "----------\n",
      "47 Train Loss: 0.0920, Train Acc: 0.9707\n",
      "47 Val Loss: 0.2963, Val Acc: 0.8816\n",
      "Epoch 48/200\n",
      "----------\n",
      "48 Train Loss: 0.0716, Train Acc: 0.9777\n",
      "48 Val Loss: 0.0507, Val Acc: 0.9882\n",
      "Epoch 49/200\n",
      "----------\n",
      "49 Train Loss: 0.0589, Train Acc: 0.9818\n",
      "49 Val Loss: 1.5064, Val Acc: 0.6961\n",
      "Epoch 50/200\n",
      "----------\n",
      "50 Train Loss: 0.0764, Train Acc: 0.9756\n",
      "50 Val Loss: 0.1240, Val Acc: 0.9632\n",
      "Epoch 51/200\n",
      "----------\n",
      "51 Train Loss: 0.0590, Train Acc: 0.9821\n",
      "51 Val Loss: 0.1236, Val Acc: 0.9513\n",
      "Epoch 52/200\n",
      "----------\n",
      "52 Train Loss: 0.0566, Train Acc: 0.9817\n",
      "52 Val Loss: 0.0367, Val Acc: 0.9895\n",
      "Epoch 53/200\n",
      "----------\n",
      "53 Train Loss: 0.0554, Train Acc: 0.9844\n",
      "53 Val Loss: 0.0780, Val Acc: 0.9776\n",
      "Epoch 54/200\n",
      "----------\n",
      "54 Train Loss: 0.0761, Train Acc: 0.9759\n",
      "54 Val Loss: 0.6599, Val Acc: 0.8053\n",
      "Epoch 55/200\n",
      "----------\n",
      "55 Train Loss: 0.0704, Train Acc: 0.9776\n",
      "55 Val Loss: 0.0899, Val Acc: 0.9711\n",
      "Epoch 56/200\n",
      "----------\n",
      "56 Train Loss: 0.0391, Train Acc: 0.9893\n",
      "56 Val Loss: 0.0732, Val Acc: 0.9737\n",
      "Epoch 57/200\n",
      "----------\n",
      "57 Train Loss: 0.0905, Train Acc: 0.9722\n",
      "57 Val Loss: 0.4064, Val Acc: 0.8724\n",
      "Epoch 58/200\n",
      "----------\n",
      "58 Train Loss: 0.0652, Train Acc: 0.9811\n",
      "58 Val Loss: 0.0752, Val Acc: 0.9724\n",
      "Epoch 59/200\n",
      "----------\n",
      "59 Train Loss: 0.0450, Train Acc: 0.9885\n",
      "59 Val Loss: 0.0265, Val Acc: 0.9934\n",
      "Epoch 60/200\n",
      "----------\n",
      "60 Train Loss: 0.0379, Train Acc: 0.9898\n",
      "60 Val Loss: 0.0332, Val Acc: 0.9921\n",
      "Epoch 61/200\n",
      "----------\n",
      "61 Train Loss: 0.0376, Train Acc: 0.9902\n",
      "61 Val Loss: 0.9389, Val Acc: 0.7921\n",
      "Epoch 62/200\n",
      "----------\n",
      "62 Train Loss: 0.0618, Train Acc: 0.9808\n",
      "62 Val Loss: 0.1027, Val Acc: 0.9539\n",
      "Epoch 63/200\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "\n",
    "model = MyCnnModel()\n",
    "\n",
    "#optimizer= torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "model, train_loss, train_acc, val_loss, val_acc = train_model(model, train_loader, 0.90, criterion, optimizer,num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Plot the training and validation losses as a function of the number of epochs\n",
    "plt.plot(train_loss, label='Train')\n",
    "plt.plot(val_loss, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training and validation accuracies as a function of the number of epochs\n",
    "plt.plot(train_acc, label='Train')\n",
    "plt.plot(val_acc, label='Validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1q2O4aOMCmoM"
   },
   "outputs": [],
   "source": [
    "#save the best model weights to a file\n",
    "torch.save(model.state_dict(), '/Users/rongwang/Desktop/ESR_project')\n",
    "\n",
    "\n",
    "# load the model\n",
    "model = MyCnnModel()\n",
    "model.load_state_dict(torch.load('/Users/rongwang/Desktop/ESR_project'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "La3yN8aGhJXy"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "output= model(dev_data_X)\n",
    "pre_lab = torch.argmax(output,dim=1)\n",
    "\n",
    "# Convert predicted labels to {valence, activation} format using the label dictionary\n",
    "pre_lab = pre_lab.tolist()\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Define label dictionary for converting labels to {valence, activation} format\n",
    "\n",
    "label_dict = {0: {\"valence\": 1, \"activation\": 1},\n",
    "              1: {\"valence\": 1, \"activation\": 0},\n",
    "              2: {\"valence\": 0, \"activation\": 0},\n",
    "              3: {\"valence\": 0, \"activation\": 1}}\n",
    "\n",
    "# Convert predicted labels to {valence, activation} format using the label dictionary\n",
    "\n",
    "# Write predicted labels to a JSON file with line numbers and {valence, activation} format\n",
    "with open(\"predicted_for_dev_200_90.json\", \"w\") as file:\n",
    "    data = {}\n",
    "    for i, label in enumerate(pre_lab):\n",
    "        data[str(i)] = label_dict[label]\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "test_file = 'test.json'\n",
    "with open(test_file, 'r') as f:\n",
    "         df=pd.read_json(f)\n",
    "df=df.T\n",
    "features = df[\"features\"].values\n",
    "lengths = [len(seq) for seq in features]\n",
    "desired_seq_length = 500\n",
    "padded_x = np.zeros((len(features), desired_seq_length, len(features[0][0])))\n",
    "# loop through each sequence in x\n",
    "for i, seq in enumerate(features):\n",
    "    # check if sequence length is greater than desired length\n",
    "      if len(seq) > desired_seq_length:\n",
    "          padded_x[i] = seq[:desired_seq_length]\n",
    "      else:\n",
    "          padded_x[i, :len(seq), :] = seq\n",
    "    # sequence length is already equal to desired length\n",
    "\n",
    "test_data_X = torch.from_numpy(padded_x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "output= model(test_data_X)\n",
    "pred_lab = torch.argmax(output,dim=1)\n",
    "\n",
    "# Convert predicted labels to {valence, activation} format using the label dictionary\n",
    "pred_lab = pre_lab.tolist()\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Define label dictionary for converting labels to {valence, activation} format\n",
    "\n",
    "label_dict = {0: {\"valence\": 1, \"activation\": 1},\n",
    "              1: {\"valence\": 1, \"activation\": 0},\n",
    "              2: {\"valence\": 0, \"activation\": 0},\n",
    "              3: {\"valence\": 0, \"activation\": 1}}\n",
    "\n",
    "# Convert predicted labels to {valence, activation} format using the label dictionary\n",
    "\n",
    "# Write predicted labels to a JSON file with line numbers and {valence, activation} format\n",
    "with open(\"prediction_test_data\", \"w\") as file:\n",
    "    data = {}\n",
    "    for i, label in enumerate(pred_lab):\n",
    "        data[str(i)] = label_dict[label]\n",
    "    json.dump(data, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets \n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class anotherCNN(nn.Module):\n",
    "    \n",
    "   def __init__(self, input_dim, in_channels=3, num_classes=10):\n",
    "        super(anotherCNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 8, kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=3, stride=2))\n",
    "            # Defining another 2D convolution layer\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(8, 32, kernel_size=3, stride=1,padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=3, stride=2))\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.dropout = nn.Dropout2d(p=0.2)\n",
    "        \n",
    "        self.fc = nn.Linear(int((input_dim/2)**2 * 64), num_classes)\n",
    "       \n",
    "        \n",
    "        self.init_weights()\n",
    "    \n",
    "   def init_weights(self):\n",
    "         for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "            \n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            \n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "   def forward(self, x):\n",
    "        #print(x.shape)  # print input shape\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "       # print(\"end of CNN\", x.shape)  # print after avgpool3 layer\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= anotherCNN(356,3,10)\n",
    "\n",
    "\n",
    "#optimizer= torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "model, train_loss, train_acc, val_loss, val_acc = train_model(model, train_loader, 0.90, criterion, optimizer,num_epochs=200)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
